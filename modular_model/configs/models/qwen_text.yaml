# Qwen3-14B text model configuration
# ALL PARAMETERS ARE REQUIRED - NO DEFAULTS PROVIDED
name: qwen3-14b
model_id: Qwen3-14B-unsloth-bnb-4bit

# vLLM Server Configuration - SINGLE SOURCE OF TRUTH
# ALL vLLM SETTINGS ARE MANDATORY
vllm_settings:
  # Basic server settings - REQUIRED
  host: 0.0.0.0
  port: 8001
  model_path: ./models/Qwen3-14B-unsloth-bnb-4bit
  served_model_name: Qwen3-14B-unsloth-bnb-4bit
  
  # Memory and performance settings - REQUIRED
  max_model_len: 27648
  gpu_memory_utilization: 0.75
  max_num_seqs: 1
  tensor_parallel_size: 1
  swap_space: 16
  
  # Model loading settings - REQUIRED
  quantization: bitsandbytes
  load_format: bitsandbytes
  dtype: auto
  trust_remote_code: true
  
  # Optimization flags - REQUIRED
  disable_log_requests: true
  disable_custom_all_reduce: true
  enforce_eager: true
  
  # Environment variables - REQUIRED
  environment:
    PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True,max_split_size_mb:128"
    VLLM_SKIP_P2P_CHECK: "1"
    TRANSFORMERS_OFFLINE: "1"
    CUDA_LAUNCH_BLOCKING: "1"
    PYTORCH_NO_CUDA_MEMORY_CACHING: "0"
    VLLM_DISABLE_CUDA_GRAPHS: "1"
    VLLM_USE_V1: "0"
    VLLM_DISABLE_COMPILATION: "1"
    TORCH_COMPILE_DISABLE: "1"
    VLLM_WORKER_MULTIPROC_METHOD: "spawn"

# Application Settings - REQUIRED
auto_start: true
timeout: 60

# Inference Parameters - ALL REQUIRED
model_params:
  temperature: 0.6
  max_tokens: 4096
  top_p: 0.95
  top_k: 20
  repetition_penalty: 1.1
  frequency_penalty: 0.0
  presence_penalty: 0.0

# System Configuration - REQUIRED
system_prompt: "You are Qwen, a helpful AI assistant."

# Performance Tuning - REQUIRED
batch_size: 1
max_concurrent_requests: 1

# Capabilities - REQUIRED
capabilities:
  vision: false
  streaming: true

# Auto-derived Properties (read-only - computed from vllm_settings)
# These are automatically set by the adapter code:
# - server_url: http://localhost:{vllm_settings.port}
# - model_path: {vllm_settings.model_path} 
# - context_window: {vllm_settings.max_model_len}