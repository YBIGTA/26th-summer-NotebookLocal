# Qwen2.5-VL vision model configuration
name: qwen2.5-vl
model_id: Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit

# vLLM Server Configuration - SINGLE SOURCE OF TRUTH
vllm_settings:
  # Basic server settings
  host: 0.0.0.0
  port: 8002
  model_path: ./models/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit
  served_model_name: Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit
  
  # Memory and performance settings
  max_model_len: 32768               # Context window (server-side)
  gpu_memory_utilization: 0.65       # GPU memory usage (65%)
  max_num_seqs: 16                   # Concurrent sequences for vision
  tensor_parallel_size: 1            # Single GPU
  max_images_per_prompt: 16          # Vision-specific setting
  
  # Model loading settings
  quantization: bitsandbytes         # Use BitsAndBytes quantization
  load_format: bitsandbytes          # Loading format
  dtype: auto                        # Auto-detect data type
  trust_remote_code: true            # Allow custom model code
  
  # Optimization flags
  disable_log_requests: true         # Reduce logging overhead
  
  # Environment variables
  environment:
    PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"  # Fix CUDA fragmentation
    VLLM_SKIP_P2P_CHECK: "1"                           # Skip P2P checks
    TRANSFORMERS_OFFLINE: "1"                          # Offline mode

# Application Settings
auto_start: true                     # Auto-start vLLM server if not running
timeout: 120                         # Request timeout (seconds)

# Inference Parameters (client-side defaults)
model_params:
  temperature: 0.7                   # Sampling temperature
  max_tokens: 8192                   # Max output tokens
  top_p: 0.9                         # Nucleus sampling
  top_k: 20                          # Top-K sampling
  repetition_penalty: 1.1            # Repetition penalty
  frequency_penalty: 0.0             # Frequency penalty
  presence_penalty: 0.0              # Presence penalty

# System Configuration
system_prompt: "You are Qwen, a helpful AI assistant with vision capabilities."

# Performance Tuning
batch_size: 2                        # Batch processing size
max_concurrent_requests: 2           # Max concurrent requests

# Vision settings
max_image_size: 1024
supported_formats:
  - jpg
  - jpeg
  - png
  - webp
  - gif

# Image processing
image_preprocessing:
  resize: true
  max_dimension: 1024
  quality: 95

# Capabilities
capabilities:
  vision: true                       # Vision model
  ocr: true                          # OCR capability
  streaming: true                    # Supports streaming

# Auto-derived Properties (read-only - computed from vllm_settings)
# These are automatically set by the adapter code:
# - server_url: http://localhost:{vllm_settings.port}
# - model_path: {vllm_settings.model_path} 
# - context_window: {vllm_settings.max_model_len}