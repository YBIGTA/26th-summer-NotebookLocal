# Qwen3-14B local text model configuration  
name: Qwen3-14B-Instruct-bnb-4bit
provider: qwen
model_id: Qwen3-14B-Instruct-bnb-4bit

# Model capabilities
capabilities:
  chat: true
  vision: false
  streaming: true
  embeddings: false

# Local model settings
model_path: "./models/Qwen3-14B-Instruct-bnb-4bit"
served_model_name: "Qwen3-14B-Instruct-bnb-4bit"

# vLLM server settings
server:
  port: 8001
  host: "127.0.0.1"
  auto_start: true
  timeout: 60

# vLLM configuration
vllm_config:
  max_model_len: 32768
  quantization: "bitsandbytes"
  load_format: "bitsandbytes"
  gpu_memory_utilization: 0.8
  dtype: "auto"
  max_num_seqs: 16
  tensor_parallel_size: 1
  trust_remote_code: true
  disable_log_requests: true
  disable_custom_all_reduce: true
  enforce_eager: true

# Model parameters
temperature: 0.7
max_tokens: 2048
top_p: 1.0
top_k: 50
repetition_penalty: 1.0
frequency_penalty: 0.0
presence_penalty: 0.0

# Environment variables for vLLM
environment:
  PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128"
  VLLM_SKIP_P2P_CHECK: "1"
  TRANSFORMERS_OFFLINE: "1"
  CUDA_LAUNCH_BLOCKING: "0"
  PYTORCH_NO_CUDA_MEMORY_CACHING: "1"
  VLLM_DISABLE_CUDA_GRAPHS: "1"
  VLLM_USE_V1: "0"
  VLLM_DISABLE_COMPILATION: "1"
  TORCH_COMPILE_DISABLE: "1"
  VLLM_WORKER_MULTIPROC_METHOD: "spawn"

# Context window
context_window: 32768

# Model description
description: "Local Qwen3-14B model optimized for Korean language"
use_cases: ["korean_text", "local_inference", "privacy_focused"]

# Workflow-specific configurations
workflows:
  qa_workflow:
    system_prompt: |
      주어진 컨텍스트를 바탕으로 질문에 답하세요. 한국어 질문에는 한국어로 답하고, 영어 질문에는 영어로 답하세요.
      Answer the question based on the provided context. Answer Korean questions in Korean and English questions in English.
      컨텍스트에서 답을 찾을 수 없다면 "제공된 컨텍스트에서 이에 대한 정보를 찾을 수 없습니다"라고 말하세요.
    parameters:
      temperature: 0.8  # Higher creativity for local model
      max_tokens: 2048
      top_p: 0.95
      top_k: 40
      repetition_penalty: 1.1
      frequency_penalty: 0.1
      presence_penalty: 0.0
    retrieval:
      preferred_k: 4
      context_strategy: "multilingual"  # Handle Korean/English mixed content
    post_processing:
      max_answer_length: 3000
      add_sources: true
      handle_korean: true
      
  summarization_workflow:
    system_prompt: |
      제공된 내용을 간결하고 포괄적으로 요약하세요. 주요 정보를 유지하면서 핵심 내용을 추출하세요.
      Summarize the provided content concisely and comprehensively, maintaining key information.
    parameters:
      temperature: 0.6
      max_tokens: 1024
      top_p: 0.9
      repetition_penalty: 1.1