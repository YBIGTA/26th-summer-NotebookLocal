# Qwen3-14B local text model configuration  
name: Qwen3-14B-Instruct-bnb-4bit
provider: qwen
model_id: Qwen3-14B-Instruct-bnb-4bit

# Model capabilities
capabilities:
  chat: true
  vision: false
  streaming: true
  embeddings: false

# Local model settings
model_path: "./models/Qwen3-14B-Instruct-bnb-4bit"
served_model_name: "Qwen3-14B-Instruct-bnb-4bit"

# vLLM server settings
server:
  port: 8001
  host: "127.0.0.1"
  auto_start: true
  timeout: 60

# vLLM configuration
vllm_config:
  max_model_len: 32768
  quantization: "bitsandbytes"
  load_format: "bitsandbytes"
  gpu_memory_utilization: 0.75
  dtype: "auto"
  max_num_seqs: 2
  tensor_parallel_size: 1
  trust_remote_code: true
  disable_log_requests: true
  disable_custom_all_reduce: true
  enforce_eager: true

# Model parameters
temperature: 0.7
max_tokens: 2048
top_p: 0.9
top_k: 20
repetition_penalty: 1.0
frequency_penalty: 0.0
presence_penalty: 0.0

# Environment variables for vLLM
environment:
  PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128"
  VLLM_SKIP_P2P_CHECK: "1"
  TRANSFORMERS_OFFLINE: "1"
  CUDA_LAUNCH_BLOCKING: "0"
  PYTORCH_NO_CUDA_MEMORY_CACHING: "1"
  VLLM_DISABLE_CUDA_GRAPHS: "1"
  VLLM_USE_V1: "0"
  VLLM_DISABLE_COMPILATION: "1"
  TORCH_COMPILE_DISABLE: "1"
  VLLM_WORKER_MULTIPROC_METHOD: "spawn"

# Context window
context_window: 32768

# Model description
description: "Local Qwen3-14B model for general text generation"
use_cases: ["local_inference", "privacy_focused", "general_text"]

# Legacy workflow configurations removed - now handled by intelligence system
# See configs/intelligence.yaml for the new configuration approach