# Qwen2.5-VL-7B vision model configuration
name: Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit
provider: qwen
model_id: Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit

# Model capabilities
capabilities:
  chat: true
  vision: true
  function_calling: true
  streaming: true
  embeddings: false

# Local model settings
model_path: "./models/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit"
served_model_name: "Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit"

# vLLM server settings
server:
  port: 8002
  host: "127.0.0.1"
  auto_start: true
  timeout: 120  # Vision models need more time

# vLLM configuration (vision optimized)
vllm_config:
  max_model_len: 32768
  quantization: "bitsandbytes"
  load_format: "bitsandbytes"
  gpu_memory_utilization: 0.7  # Leave more room for vision processing
  dtype: "auto"
  max_num_seqs: 8  # Lower for vision models
  tensor_parallel_size: 1
  trust_remote_code: true
  disable_log_requests: true
  disable_custom_all_reduce: true
  enforce_eager: true

# Vision-specific settings
vision:
  max_image_size: 1024
  supported_formats: ["jpg", "png", "webp", "jpeg"]
  max_images_per_request: 4

# Model parameters
temperature: 0.7
max_tokens: 2048
top_p: 1.0
top_k: 50
repetition_penalty: 1.0
frequency_penalty: 0.0
presence_penalty: 0.0

# Environment variables for vLLM
environment:
  PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128"
  VLLM_SKIP_P2P_CHECK: "1"
  TRANSFORMERS_OFFLINE: "1"
  CUDA_LAUNCH_BLOCKING: "0"
  PYTORCH_NO_CUDA_MEMORY_CACHING: "1"
  VLLM_DISABLE_CUDA_GRAPHS: "1"
  VLLM_USE_V1: "0"
  VLLM_DISABLE_COMPILATION: "1"
  TORCH_COMPILE_DISABLE: "1"
  VLLM_WORKER_MULTIPROC_METHOD: "spawn"

# Context window
context_window: 32768

# Cost information (local model - no API costs)
cost:
  input: 0.0
  output: 0.0

# Model description
description: "Local Qwen2.5-VL vision model for multimodal understanding"
use_cases: ["vision_tasks", "image_analysis", "multimodal_chat", "local_inference"]