# NotebookLocal - Multi-Modal RAG System

## What We've Built

A comprehensive **multi-modal RAG (Retrieval-Augmented Generation) system** with three integrated components:

### üéØ **Core Components**
- **üì° Inference Server**: FastAPI backend for document processing and Q&A
- **ü§ñ Modular Model Router**: Unified API gateway for multiple LLM providers (OpenAI, Anthropic, Qwen, vLLM)
- **üìù Obsidian Plugin**: Native frontend interface within Obsidian

## What We're Trying To Do

### üéØ **Primary Goals**
1. **üá∞üá∑ Korean PDF Support**: Advanced document processing for Korean academic/technical documents
2. **üîÑ Multi-Modal AI**: Combine text, images, and diagrams for comprehensive understanding
3. **üöÄ Flexible LLM Access**: Support both cloud APIs and local models through unified interface
4. **üìö Knowledge Management**: Seamless integration with personal note-taking workflows

### üéØ **Vision**
Create a **notebook-local AI assistant** that:
- Processes complex Korean documents (PDFs with mixed text/images)
- Provides contextual Q&A based on uploaded content
- Integrates naturally into existing knowledge workflows
- Supports both cloud and local AI models for flexibility

## System Architecture

## How We're Building It

### üèóÔ∏è **Three-Tier Architecture**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Obsidian Plugin           ‚îÇ
‚îÇ         (User Interface)            ‚îÇ
‚îÇ  ‚Ä¢ Korean document upload          ‚îÇ
‚îÇ  ‚Ä¢ Multi-modal chat interface      ‚îÇ  
‚îÇ  ‚Ä¢ Real-time streaming responses   ‚îÇ
‚îÇ  ‚Ä¢ Integrated note management      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ HTTP API
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Inference Server           ‚îÇ
‚îÇ       (Document Processing)         ‚îÇ
‚îÇ  ‚Ä¢ Korean PDF text extraction      ‚îÇ
‚îÇ  ‚Ä¢ Image/diagram processing        ‚îÇ
‚îÇ  ‚Ä¢ Vector embedding generation     ‚îÇ
‚îÇ  ‚Ä¢ RAG pipeline orchestration      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ Model API
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        Modular Model Router         ‚îÇ
‚îÇ         (AI Model Gateway)          ‚îÇ
‚îÇ  ‚Ä¢ OpenAI/Anthropic (Cloud)        ‚îÇ
‚îÇ  ‚Ä¢ Qwen-VL (Local Vision)          ‚îÇ
‚îÇ  ‚Ä¢ vLLM (Local Text)               ‚îÇ
‚îÇ  ‚Ä¢ Adaptive model routing          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üîÑ **Technical Implementation Strategy**

#### **1. Korean Document Processing**
- **PyMuPDF**: Superior Korean font handling vs. pdfplumber
- **Multi-modal extraction**: Text + images + diagrams from PDFs
- **GPT-4V integration**: AI-generated descriptions for visual content
- **Semantic chunking**: Context-aware text segmentation

#### **2. Flexible AI Backend**
- **Model abstraction**: Unified interface for different AI providers
- **Local + Cloud**: Support both self-hosted and API-based models
- **Vision capabilities**: Qwen2.5-VL for local image understanding
- **Performance optimization**: vLLM for fast local inference

#### **3. Seamless User Experience**
- **Native Obsidian integration**: Works within existing workflows
- **Real-time streaming**: Progressive response generation
- **Smart caching**: Minimize API calls and processing time
- **Error resilience**: Graceful fallbacks between model providers

## Current Development Status

### ‚úÖ **Completed Components**

#### üì° **Inference Server**
- ‚úÖ FastAPI backend with Korean PDF processing
- ‚úÖ PyMuPDF integration for better Korean font support
- ‚úÖ Multi-modal content extraction (text + images)
- ‚úÖ OpenAI integration (GPT-4o, text-embedding-3-large)
- ‚úÖ Vector storage with Weaviate
- ‚úÖ RESTful API endpoints for document processing

#### ü§ñ **Modular Model Router** 
- ‚úÖ Unified API gateway architecture
- ‚úÖ Multiple provider support (OpenAI, Anthropic, Qwen)
- ‚úÖ Local model integration (vLLM, Qwen-VL)
- ‚úÖ Configuration-driven routing
- ‚úÖ Async request handling

#### üìù **Obsidian Plugin**
- ‚úÖ TypeScript plugin architecture
- ‚úÖ React-based UI components
- ‚úÖ Real-time chat interface
- ‚úÖ Document management system
- ‚úÖ Settings configuration panel

### üöß **Technology Stack**

#### **Backend (Python)**
- **FastAPI**: High-performance async API server
- **LangChain/LangGraph**: AI workflow orchestration  
- **PyMuPDF**: Korean PDF text extraction
- **Weaviate**: Vector similarity search
- **vLLM**: Local model serving
- **Transformers**: Qwen model integration

#### **Frontend (TypeScript)**
- **Obsidian API**: Native plugin integration
- **React + TypeScript**: Component architecture
- **Tailwind CSS**: Responsive UI design
- **esbuild**: Fast compilation pipeline

## Next Steps & Testing

### üéØ **Immediate Goals**
1. **End-to-end testing**: Verify complete workflow functionality
2. **Korean document validation**: Test with real Korean academic PDFs
3. **Performance optimization**: Latency and memory usage improvements
4. **Integration polish**: Smooth user experience across components

### üìã **Testing Checklist**
- [ ] Obsidian plugin builds and loads correctly
- [ ] Inference server processes Korean PDFs successfully  
- [ ] Model router switches between providers seamlessly
- [ ] Complete document ‚Üí query ‚Üí response workflow
- [ ] Multi-modal content (text + images) processing
- [ ] Real-time streaming responses

### üîÑ **Processing Pipeline**

```mermaid
graph TD
    A[Korean PDF Upload] --> B[PyMuPDF Extraction]
    B --> C{Content Analysis}
    C -->|Text| D[Korean Text Processing]
    C -->|Images| E[Image Extraction]
    E --> F[Qwen-VL Description]
    D --> G[Semantic Chunking]
    F --> G
    G --> H[Embedding Generation]
    H --> I[Vector Storage]
    I --> J[RAG Index Ready]
    J --> K[User Query]
    K --> L[Similarity Search]
    L --> M[Context Assembly]
    M --> N[LLM Response Generation]
    N --> O[Streamed Response]
```

## Getting Started

### üöÄ **Quick Setup Guide**

#### 1. **Environment Setup**
```bash
# Ensure Python 3.12 is installed
python3 --version  # Should be 3.12.x

# Clone and navigate
git clone <repository>
cd 26th-summer-NotebookLocal/
```

#### 2. **Start Model Router** (Optional - for local models)
```bash
cd modular_model/
python -m venv venv && source venv/bin/activate
pip install -e .
python src/main.py  # Starts on port 8001
```

#### 3. **Start Inference Server**
```bash
cd inference-server/
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env: Add your OPENAI_API_KEY

# Start server
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

#### 4. **Install Obsidian Plugin**
```bash
cd obsidian-plugin/
npm install && npm run build

# Copy to Obsidian plugins directory
mkdir -p ~/.obsidian/plugins/obsidian-copilot/
cp main.js manifest.json styles.css ~/.obsidian/plugins/obsidian-copilot/

# Enable in Obsidian: Settings ‚Üí Community Plugins ‚Üí Enable "Obsidian Copilot"
```

### üß™ **Testing the System**
See [`TESTING_GUIDE.md`](./TESTING_GUIDE.md) for comprehensive testing instructions.

## Project Structure

```
rag-search-pipeline/
‚îú‚îÄ‚îÄ README.md                           # This file - project overview
‚îÇ
‚îú‚îÄ‚îÄ inference-server/                   # Backend AI processing server
‚îÇ   ‚îú‚îÄ‚îÄ README.md                       # Server setup and deployment guide
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ config.py                       # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ setup.py                        # Automated setup script
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/                           # FastAPI application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # Server entry point
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py                  # API endpoint definitions
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ src/                           # Core processing logic
‚îÇ       ‚îú‚îÄ‚îÄ processors/                # Document processing
‚îÇ       ‚îú‚îÄ‚îÄ storage/                   # Vector storage
‚îÇ       ‚îú‚îÄ‚îÄ workflows/                 # LangChain workflows
‚îÇ       ‚îî‚îÄ‚îÄ utils/                     # Utilities and helpers
‚îÇ
‚îî‚îÄ‚îÄ obsidian-plugin/                   # Frontend Obsidian integration
    ‚îú‚îÄ‚îÄ README.md                      # Plugin installation and usage
    ‚îú‚îÄ‚îÄ package.json                   # Node.js dependencies
    ‚îú‚îÄ‚îÄ manifest.json                  # Obsidian plugin metadata
    ‚îú‚îÄ‚îÄ tsconfig.json                  # TypeScript configuration
    ‚îÇ
    ‚îî‚îÄ‚îÄ src/                          # Plugin source code
        ‚îú‚îÄ‚îÄ main.ts                   # Plugin entry point
        ‚îú‚îÄ‚îÄ api/                      # Server communication
        ‚îú‚îÄ‚îÄ components/               # UI components
        ‚îú‚îÄ‚îÄ settings/                 # Configuration interface
        ‚îî‚îÄ‚îÄ utils/                    # Plugin utilities
```

## Key Features & Differentiators

### üá∞üá∑ **Korean Document Excellence**
- **Advanced font handling**: PyMuPDF > pdfplumber for Korean text
- **Multi-modal processing**: Text + images + diagrams in single workflow
- **Academic PDF focus**: Optimized for Korean research/technical documents

### ü§ñ **Flexible AI Architecture**
- **Multi-provider support**: Switch between OpenAI, Anthropic, local models
- **Local model integration**: Qwen-VL for vision, vLLM for fast inference
- **Smart routing**: Route queries to most appropriate model for the task

### üìù **Native Workflow Integration**
- **Obsidian-first design**: Works within existing note-taking habits
- **Real-time streaming**: Progressive response generation
- **Source transparency**: Clear document references for every response

## Component Details

### üì° **Inference Server Features**
- Korean PDF processing with PyMuPDF
- Multi-modal content extraction (text + images)
- Vector storage with Weaviate
- LangChain workflow orchestration
- RESTful API endpoints

### ü§ñ **Model Router Features**
- Unified API for multiple providers
- Local model serving (Qwen, vLLM)
- Configuration-driven routing
- Async request handling

### üìù **Obsidian Plugin Features**
- Native Obsidian integration
- Real-time chat interface
- Document management system
- Settings configuration
- Source reference tracking

---

## Development Team & Contact

**Built for the 26th Summer Research Program**
- Multi-modal RAG system for Korean academic document processing
- Local + cloud AI model integration
- Obsidian workflow optimization

For detailed setup and testing instructions, see:
- [`TESTING_GUIDE.md`](./TESTING_GUIDE.md) - Comprehensive testing procedures
- [`inference-server/README.md`](./inference-server/README.md) - Server setup and API docs
- [`obsidian-plugin/README.md`](./obsidian-plugin/README.md) - Plugin installation guide
- [`modular_model/README.md`](./modular_model/README.md) - Model router documentation